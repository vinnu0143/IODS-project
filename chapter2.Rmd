# Regression and model validation
This week we start the data wrangling, do some exploratory examination of the data and fit a simple linear model to the data.

##Reading data
Code for data creation is available at:
https://github.com/rsund/IODS-project/blob/master/data/create_learning2014.R

Letâ€™s read the data in and make sure that gender is converted to factor

``` {r readdata,echo=TRUE,results='hide',message=FALSE,warning=FALSE}
setwd("~/IODS-project")
library(dplyr)

learning2014 <- readxl::read_excel("~/IODS-project/data/learning2014.xlsx") %>% mutate_at(vars(gender), factor)
```
As can be seen, the structure looks correct now

``` {r}
str(learning2014)

```

There are 166 observations of 7 variables of which the gender is converted to factor. Other 6 variables contain integer values

## Exploring data

Let's draw some figures to see how the data looks
```{r fig1, fig.path="figures/"}
pairs(learning2014[!names(learning2014) %in% c("gender")],col=learning2014$gender)
```

The above pairwise comparison between the variables with male and female gender shown in black and red. There is no clear seperation or correlation between pairs as seen in the graphs. This can be further explained by ggpairs graphs. 
```{r fig2, fig.path="figures/", fig.dim=c(10,10), results='hide', message=FALSE}
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
ggpairs(learning2014, 
        mapping = aes(col = gender, alpha = 0.3), 
        lower = list(combo = wrap("facethist", bins = 20))
        )
```

In pairs we are limited to scatter plots but in ggpairs we can visualize various plots for numerical or categorical varibles. From the above graphs highest correlation can be seen between attitude and points. 

## Linear regression

The highest correlation is between *attitude* and *points*, **Cor:** `r cor(learning2014$attitude,learning2014$points)`.
Let's take a closer look.

```{r}
qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
```

Let's fit a linear model to the data. Points are explained by attitude.
The equation for the model is
$$
Y_i = \alpha + \beta_1 X_i + \epsilon_i
$$
where Y represent points, X is attitude, $\alpha$ is constant, $\beta_1$ is regression
coefficient for attitude, and $\epsilon$ is a random term.

Estimation of the model yields the following results:
```{r, results='asis'}
my_model <- lm(points ~ attitude, data = learning2014)
results <- summary(my_model)
knitr::kable(results$coefficients, digits=3, caption="Regression coefficients")
```

Intercept as well as attitude are statistically significant predictors. 
Coefficient of determination $R^2$ = `r results$r.squared` that is not particularly high.
Probably some more predictors could be added to the model.

## Multiple Linear regression

```{r, results='asis'}
my_model2 <- lm(points ~ attitude + stra + surf, data = learning2014)
results2 <- summary(my_model2)
knitr::kable(results2$coefficients, digits=3, caption="Regression coefficients2")
```

Intercept as well as attitude are statistically significant predictors. But the stra and surf are not statistically significant.
Coefficient of determination $R^2$ = `r results2$r.squared` that is not particularly high.

### Diagnostic plots
```{r fig3, fig.path="figures/"}
plot(my_model2, which=c(1,2,5))
```
The residual vs fitted values show good random distributoion and is linear reasonably.


