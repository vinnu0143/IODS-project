# Regression and model validation
This week we start the data wrangling, do some exploratory examination of the data and fit a simple linear model to the data.

##Reading data
Code for data creation is available at:
https://github.com/rsund/IODS-project/blob/master/data/create_learning2014.R

Letâ€™s read the data in and make sure that gender is converted to factor

``` {r readdata,echo=TRUE,results='hide',message=FALSE,warning=FALSE}
setwd("~/IODS-project")
library(dplyr)

learning2014 <- readxl::read_excel("~/IODS-project/data/learning2014.xlsx") %>% mutate_at(vars(gender), factor)
```
As can be seen, the structure looks correct now

``` {r}
str(learning2014)

```

There are 166 observations of 7 variables of which the gender is converted to factor. Other 6 variables contain integer values

## Exploring data

Let's draw some figures to see how the data looks
```{r fig1, fig.path="figures/"}
pairs(learning2014[!names(learning2014) %in% c("gender")],col=learning2014$gender)
```

The above pairwise comparison between the variables with male and female gender shown in black and red. There is no clear seperation or correlation between pairs as seen in the graphs. This can be further explained by ggpairs graphs. 
```{r fig2, fig.path="figures/", fig.dim=c(10,10), results='hide', message=FALSE}
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
ggpairs(learning2014, 
        mapping = aes(col = gender, alpha = 0.3), 
        lower = list(combo = wrap("facethist", bins = 20))
        )
```

In pairs we are limited to scatter plots but in ggpairs we can visualize various plots for numerical or categorical varibles. From the above graphs highest correlation can be seen between attitude and points. 

## Linear regression

The highest correlation is between *attitude* and *points*, **Cor:** `r cor(learning2014$attitude,learning2014$points)`.
Let's take a closer look.

```{r}
qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
```

Let's fit a linear model to the data. Points are explained by attitude.
The equation for the model is
$$
Y_i = \alpha + \beta_1 X_i + \epsilon_i
$$
where Y represent points, X is attitude, $\alpha$ is constant, $\beta_1$ is regression
coefficient for attitude, and $\epsilon$ is a random term.

Estimation of the model yields the following results:
```{r, results='asis'}
my_model <- lm(points ~ attitude, data = learning2014)
results <- summary(my_model)
knitr::kable(results$coefficients, digits=3, caption="Regression coefficients")
```

Intercept as well as attitude are statistically significant predictors. 
Coefficient of determination $R^2$ = `r results$r.squared` which is not so high. But what we can interpret from the model is that for every 3.525 increase in attitude leads to 11.637 increase in points. There is a positive correlation etween attitude and points. 
Probably some more predictors could be added to the model.

## Multiple Linear regression

```{r, results='asis'}
my_model2 <- lm(points ~ attitude + stra + surf, data = learning2014)
results2 <- summary(my_model2)
knitr::kable(results2$coefficients, digits=3, caption="Regression coefficients2")
```

Intercept as well as attitude are statistically significant predictors. But the stra and surf are not statistically significant. Surf shows negative correlation with points, whereas stra shows positive correlation along with attitude against points. All combined there is a small improvement in coefficient of determination from using one variable to three. I guess this is the maximum correlation one can get for this dataset. 

From the intercept we can infer that for every 3.395 increase in attitude, o.853 increase in stra and 0.586 decrease in surf there is an increase of 11.017 of points. 

Coefficient of determination $R^2$ = `r results2$r.squared` which is not high but .

### Diagnostic plots

The diagnostic plots can reveal further the quality of the model. 

```{r fig3, fig.path="figures/"}
plot(my_model2, which=c(1,2,5))
```

From the residuals vs fitted plot we can assume that 

The residuals bounce randomly around the 0 line, this suggests that the relationship is linear. There are some values which are very far and is the cause for the line not be a striaght line. These are kind of outliers which raise errors in the dataset.

From the QQ plot one can see that most of the values fall on the straight line. There are some outliers which cause randomness in the dataset. 

From the Reiduals vs levarage plot most of the values fall away from the cooks distance but some values like 145, 35,75 looks like they are outliers.

From all the multiple linear regression assessment we can see that the cofficient of determination is too low to consider the model. I would suggest that we have more data for bulding a good model.


